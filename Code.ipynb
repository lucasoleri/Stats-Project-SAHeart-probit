{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf3e4e",
   "metadata": {
    "id": "bbbf3e4e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "#import statsmodels.api as sm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521d3510",
   "metadata": {
    "id": "521d3510"
   },
   "source": [
    "# Loading and transforming the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80157b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "a80157b9",
    "outputId": "13005eb0-480a-462c-d6da-e9dd6caab1ad"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"SAheart.txt\",delim_whitespace=True)\n",
    "df.loc[df[\"famhist\"] == \"Present\", \"famhist\"] = 1\n",
    "df.loc[df[\"famhist\"] == \"Absent\", \"famhist\"] = 0\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5463dce7",
   "metadata": {
    "id": "5463dce7"
   },
   "source": [
    "# Splitting data into independet and dependent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mkXDbURPd5CC",
   "metadata": {
    "id": "mkXDbURPd5CC"
   },
   "outputs": [],
   "source": [
    "Y = df[\"chd\"]\n",
    "X = df.drop([\"chd\"],axis=1)\n",
    "X = (X - X.mean(axis=0))/X.std(axis=0) #standardizing the independent variables\n",
    "X = X.astype(float).to_numpy()\n",
    "Y = Y.astype(float).to_numpy()\n",
    "p = X.shape[1]\n",
    "n = X.shape[0]\n",
    "Y = Y.reshape((n, 1))\n",
    "\n",
    "X_1 = np.ones(n, dtype=int) #intercept\n",
    "X_1 = np.reshape(X_1, (n, 1))\n",
    "X = np.hstack((X_1, X)) #gaussian covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec81853",
   "metadata": {
    "id": "fec81853"
   },
   "outputs": [],
   "source": [
    "#Y = df[\"chd\"]\n",
    "#X = df.drop([\"chd\"],axis=1)\n",
    "#X = (X - X.mean(axis=0))/X.std(axis=0) #standardizing the independent variables\n",
    "#X = sm.add_constant(X) # add constant vector for the intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50a85d5",
   "metadata": {
    "id": "c50a85d5"
   },
   "source": [
    "# Reference model (uses Statsmodel) -- jusk for checking our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5861caf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5861caf",
    "outputId": "68333328-9090-4228-9735-5faddbf34d7b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.discrete.discrete_model import Probit\n",
    "\n",
    "model = Probit(Y, X.astype(float))\n",
    "probit_model = model.fit()\n",
    "print(probit_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbb8cb8",
   "metadata": {
    "id": "cbbb8cb8"
   },
   "source": [
    "# Probit with Fisher Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b7d1d9",
   "metadata": {
    "id": "e4b7d1d9"
   },
   "outputs": [],
   "source": [
    "def probit(X,Y, epsilon):\n",
    "#X = covariates\n",
    "#Y = dependent variable\n",
    "#epsilon = threshold for convergence\n",
    "\n",
    "    n,p = np.shape(X)\n",
    "\n",
    "    #Initial values of the algorithms\n",
    "    b_0 = np.zeros((p,1))\n",
    "    eta_0 = np.dot(X, b_0)\n",
    "    mu_0 = norm.cdf(eta_0)\n",
    "\n",
    "    W = np.diag((((norm.pdf(eta_0))**2)/(mu_0*(1-mu_0))).reshape(n,))\n",
    "    Z = eta_0+(Y-mu_0)/(norm.pdf(eta_0))\n",
    "\n",
    "\n",
    "    #boolean\n",
    "    convergence = False\n",
    "    while not convergence:\n",
    "\n",
    "        #new estimate\n",
    "        b = np.linalg.multi_dot([np.linalg.inv(np.linalg.multi_dot([X.T, W, X])), X.T,W, Z])\n",
    "        eta = np.dot(X, b)\n",
    "        mu = norm.cdf(eta)\n",
    "        W = np.diag((((norm.pdf(eta))**2)/(mu*(1-mu))).reshape(n,))\n",
    "        Z = eta+(Y-mu)/(norm.pdf(eta))\n",
    "\n",
    "        if np.linalg.norm(b-b_0)/(np.linalg.norm(b_0)+epsilon) < epsilon:\n",
    "        #relative convergence criterion\n",
    "            convergence = True\n",
    "            print(\"\\nConvergence reached with value:\",np.linalg.norm(b-b_0)/(np.linalg.norm(b_0)+epsilon))\n",
    "        b_0 = b #old value\n",
    "\n",
    "    return(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3799a9",
   "metadata": {
    "id": "aa3799a9"
   },
   "source": [
    "# Fisher Scoring model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4445c8f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "4445c8f7",
    "outputId": "27077084-7595-43bd-a7bf-e077356a2f1b"
   },
   "outputs": [],
   "source": [
    "beta = probit(X,Y,epsilon = 0.000001)\n",
    "#print(\"Beta:\", beta.reshape((p+1,))) #estimated beta\n",
    "#our own step by step model\n",
    "betas = pd.DataFrame(beta)\n",
    "betas.rename(columns={0: 'Betas'},inplace=True)\n",
    "new_index = list(df.columns)\n",
    "betas.rename(index=dict(zip(betas.index, new_index)),inplace=True)\n",
    "betas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0393ad5",
   "metadata": {
    "id": "a0393ad5"
   },
   "source": [
    "# Metropolis Hastings model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d2aa2f",
   "metadata": {
    "id": "b5d2aa2f"
   },
   "source": [
    "# Define Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ab7a76",
   "metadata": {
    "id": "e1ab7a76"
   },
   "outputs": [],
   "source": [
    "# Normal CDF: P(c <= XB)\n",
    "def normal_cdf(X, beta): #accepts vectors too\n",
    "    linear_predictor = np.array(X) @ np.array(beta)\n",
    "    #print(linear_predictor)\n",
    "    prob_vector = []\n",
    "    if isinstance(linear_predictor,float):\n",
    "        return 1 / 2 * (1 + np.math.erf(linear_predictor / np.sqrt(2)))\n",
    "    else:\n",
    "        for lin in linear_predictor:\n",
    "            prob = 1 / 2 * (1 + np.math.erf(lin / np.sqrt(2)))\n",
    "            prob_vector.append(prob)\n",
    "        return np.array(prob_vector)\n",
    "\n",
    "# Prior function\n",
    "#def prior(beta):\n",
    "#    return 1 #non informative prior\n",
    "\n",
    "def prior_gaussian(beta, scale=10):\n",
    "    # Calculate the probability density function of a normal distribution\n",
    "    pdf_values = (1 / (np.sqrt(2 * np.pi) * scale)) * np.exp(-0.5 * (beta / scale)**2)\n",
    "\n",
    "    # Compute the product of the probabilities for each parameter\n",
    "    prior_prob = np.prod(pdf_values)\n",
    "    return prior_prob\n",
    "\n",
    "def prior_uniform(beta, scale=10):\n",
    "    return 1\n",
    "\n",
    "def cauchy_pdf(x, loc=0, scale=1):\n",
    "    # Cauchy probability density function\n",
    "    return (1 / (np.pi * scale * (1 + ((x - loc) / scale)**2)))\n",
    "\n",
    "def prior_cauchy(beta, scale=10):\n",
    "    # Calculate the probability density function of a Cauchy distribution\n",
    "    pdf_values = cauchy_pdf(beta, loc=0, scale=scale)\n",
    "\n",
    "    # Compute the product of the probabilities for each parameter\n",
    "    prior_prob = np.prod(pdf_values)\n",
    "\n",
    "    return prior_prob\n",
    "\n",
    "#Log Likelyhood function -- not used in the Metroposlis Hastings\n",
    "#def log_likelihood(y, X, beta): #log_likelyhood requires sum of logs\n",
    "#    log_likelihood_term = np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
    "#    return log_likelihood_term\n",
    "\n",
    "# Likelihood function\n",
    "def likelihood(y, X, beta):\n",
    "    p = normal_cdf(X, beta)\n",
    "    likelihood_term = np.prod(p**y * (1 - p)**(1 - y))\n",
    "    return likelihood_term\n",
    "\n",
    "# Posterior function\n",
    "def posterior(beta, y, X, prior_type = 'uniform'):\n",
    "    if prior_type == 'uniform':\n",
    "      prior = prior_uniform\n",
    "    if prior_type == 'normal':\n",
    "      prior = prior_gaussian\n",
    "    if prior_type == 'cauchy':\n",
    "      prior = prior_cauchy\n",
    "    post = likelihood(y, X, beta) * prior(beta)\n",
    "    if post != 0:\n",
    "        return post\n",
    "    else:\n",
    "        return 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3512e801",
   "metadata": {
    "id": "3512e801"
   },
   "outputs": [],
   "source": [
    "# Metropolis-Hastings algorithm\n",
    "def metropolis_hastings(initial_beta, iterations, y, X, prior_type):\n",
    "    beta_samples = [initial_beta]\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        current_beta = beta_samples[-1]\n",
    "\n",
    "        # Propose a new beta using a random walk\n",
    "        proposal = current_beta + np.random.normal(0, 0.05, size=len(current_beta))\n",
    "\n",
    "        # Calculate acceptance ratio\n",
    "        acceptance_ratio = min(1, posterior(proposal, y, X, prior_type) / posterior(current_beta, y, X, prior_type))\n",
    "\n",
    "        # Accept or reject the proposed beta\n",
    "        if np.random.rand() < acceptance_ratio:\n",
    "            beta_samples.append(proposal)\n",
    "        else:\n",
    "            beta_samples.append(current_beta)\n",
    "\n",
    "    return np.array(beta_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c4453",
   "metadata": {
    "id": "a44c4453"
   },
   "source": [
    "# Run Metropolis Hastings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab2ccf",
   "metadata": {
    "id": "9dab2ccf"
   },
   "outputs": [],
   "source": [
    "#set starting value of beta to a vector of zeros\n",
    "init_beta = np.zeros(10) #np.zeros(10) + 0.01 #if working with logs\n",
    "# RUN metropolis hastings\n",
    "m = metropolis_hastings(init_beta, 10000, Y, X, 'normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7630acc7-e9ff-49e9-b922-a9ff72e30d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m[:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03143740",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03143740",
    "outputId": "671dc1eb-a349-430b-cf11-8f1c6c57ae1d"
   },
   "outputs": [],
   "source": [
    "#Mean of the last 100 sampled betas\n",
    "betas_100mean = np.mean(m[:-100],axis=0)\n",
    "betas_100mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AYzVU4gqZ5nv",
   "metadata": {
    "id": "AYzVU4gqZ5nv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#set starting value of beta to a vector of zeros\n",
    "init_beta = np.zeros(10) #np.zeros(10) + 0.01 #if working with logs\n",
    "# RUN metropolis hastings\n",
    "m_unif = metropolis_hastings(init_beta, 10000, Y, X, 'uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lv4rwFmLZ5n3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lv4rwFmLZ5n3",
    "outputId": "e17ad510-c117-44b9-9848-4f05b6478234"
   },
   "outputs": [],
   "source": [
    "#Mean of the last 100 sampled betas\n",
    "betas_100mean_unif = np.mean(m_unif[:-100],axis=0)\n",
    "betas_100mean_unif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o037d6fsaIjJ",
   "metadata": {
    "id": "o037d6fsaIjJ"
   },
   "outputs": [],
   "source": [
    "#set starting value of beta to a vector of zeros\n",
    "init_beta = np.zeros(10) #np.zeros(10) + 0.01 #if working with logs\n",
    "# RUN metropolis hastings\n",
    "m_cauchy = metropolis_hastings(init_beta, 10000, Y, X, 'uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44Y60oKEaIjK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44Y60oKEaIjK",
    "outputId": "01b4c42b-82f0-4762-a15f-34af3aa1c804"
   },
   "outputs": [],
   "source": [
    "#Mean of the last 100 sampled betas\n",
    "betas_100mean_cauchy = np.mean(m_cauchy[:-100],axis=0)\n",
    "betas_100mean_cauchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432eacfc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "432eacfc",
    "outputId": "9b7bc3d9-5dda-4f28-b8fc-0d92ce06d486"
   },
   "outputs": [],
   "source": [
    "# Display estimated betas\n",
    "betas_estimate = pd.DataFrame(betas_100mean)\n",
    "betas_estimate.rename(columns={0: 'Mean of last 100 Betas'},inplace=True)\n",
    "new_index = list(df.columns)\n",
    "betas_estimate.rename(index=dict(zip(betas_estimate.index, new_index)),inplace=True)\n",
    "betas_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dbffa6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "57dbffa6",
    "outputId": "ec3a79a6-3b82-4742-fa26-bab35512756c"
   },
   "outputs": [],
   "source": [
    "#Convergence plots for the betas\n",
    "plt.plot(m_unif)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Parameter Value')\n",
    "plt.title('Convergence Plot for the 10 Beta Parameters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79817c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6c79817c",
    "outputId": "63920ffc-80d3-4b2d-9879-a5c3387dc674"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 1, figsize=(10, 2*10))\n",
    "\n",
    "# Loop through variables and plot ACF for each beta\n",
    "for j in range(10):\n",
    "    sm.graphics.tsa.plot_acf(m[:, j], lags=40, ax=axes[j], title=f'ACF for Variable {j + 1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d5ca49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "18d5ca49",
    "outputId": "a95a2472-9904-44ba-bbab-8881a7771e01"
   },
   "outputs": [],
   "source": [
    "#Convergence as shown by ACF with thinning every 10 steps\n",
    "\n",
    "betas_burnin = m[1000:]\n",
    "fig, axes = plt.subplots(10, 1, figsize=(10, 2*10))\n",
    "\n",
    "# Loop through variables and plot ACF for each beta\n",
    "for j in range(10):\n",
    "    sm.graphics.tsa.plot_acf(betas_burnin[:, j], lags=40, ax=axes[j], title=f'ACF for Variable {j + 1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f93844",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "55f93844",
    "outputId": "e5c8526d-8792-49ca-9a5a-6ace593c5007"
   },
   "outputs": [],
   "source": [
    "#Convergence as shown by ACF with thinning every 10 steps\n",
    "\n",
    "betas_thin = m[1::10]\n",
    "fig, axes = plt.subplots(10, 1, figsize=(10, 2*10))\n",
    "\n",
    "# Loop through variables and plot ACF for each beta\n",
    "for j in range(10):\n",
    "    sm.graphics.tsa.plot_acf(betas_thin[:, j], lags=40, ax=axes[j], title=f'ACF for Variable {j + 1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
